{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import dash\n",
    "from dash import dcc as dcc\n",
    "from dash import html as html\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from dash.dependencies import Input, Output, State\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import math\n",
    "import contractions\n",
    "import string\n",
    "import networkx as nx\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all parquet files from a folder containing multiple parquet files in pandas dataframe\n",
    "named_entities = pd.read_parquet('./named_entities/', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "explode_df = named_entities.explode(\"named_entities\").rename(columns={\"named_entities\": \"entity\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the id and entity columns only\n",
    "entity_df = explode_df[['id', 'entity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_df[\"entity_name\"] = entity_df[\"entity\"].str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of unique entities from entity_name column in entity_df\n",
    "unique_entities = entity_df['entity_name'].str.lower().unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that takes an input string and returns a list of ids that contain the input string from the entity_df dataframe\n",
    "def get_ids(input_string):\n",
    "    # Replace NaN values in 'entity_name' column with empty strings\n",
    "    entity_df['entity_name'] = entity_df['entity_name'].fillna('')\n",
    "    \n",
    "    # Filter the DataFrame based on the exact input_string and return unique IDs as a list\n",
    "    return entity_df[entity_df['entity_name'].str.lower() == input_string.lower()]['id'].unique().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that takes a list of ids and returns a list of articles that contain the ids from the named_entities dataframe\n",
    "def get_articles(ids):\n",
    "    # Filter the DataFrame based on the ids list and return unique articles as a list\n",
    "    return named_entities[named_entities['id'].isin(ids)]['article'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(article):\n",
    "    \n",
    "    input_text = article\n",
    "    max_length = math.ceil(len(article)/10)\n",
    "    \n",
    "    if max_length > 1024:\n",
    "        max_length = 1024\n",
    "    elif max_length < 56:\n",
    "        max_length = 56\n",
    "    \n",
    "    inputs = tokenizer(input_text, truncation=True, padding=\"longest\", max_length=1024, return_tensors=\"pt\")\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=max_length, early_stopping=True)\n",
    "    \n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that takes a list of articles and returns a list of summaries of the articles\n",
    "def get_summaries(articles):\n",
    "    # Create an empty list to store summaries\n",
    "    summaries = []\n",
    "    \n",
    "    # Loop through all articles and append the summary to the list\n",
    "    for article in articles:\n",
    "        summaries.append(get_summary(article))\n",
    "    \n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vijay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_tf_idf_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    preprocessed_text = contractions.fix(preprocessed_text)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "def tf_score(word, sentence):\n",
    "    word_frequency_in_sentence = sentence.split().count(word)\n",
    "    len_sentence = len(sentence.split())\n",
    "    tf = word_frequency_in_sentence / len_sentence\n",
    "    return tf\n",
    "\n",
    "def idf_score(no_of_sentences, word, sentences):\n",
    "    no_of_sentence_containing_word = sum(1 for sentence in sentences if word in sentence)\n",
    "    idf = math.log10(no_of_sentences / (no_of_sentence_containing_word + 1))\n",
    "    return idf\n",
    "\n",
    "def get_tf_idf_summary(custom_text, percentage):\n",
    "\n",
    "    no_of_sentences = math.ceil(len(sent_tokenize(custom_text)) * (percentage/100))\n",
    "    sentences = sent_tokenize(custom_text)\n",
    "    \n",
    "    sentences_tf_idf = {}\n",
    "    for i, sentence in enumerate(sentences, 1):\n",
    "        sentence_tf_idf = 0\n",
    "        sentence = re.sub(r'\\d+', '', sentence)\n",
    "        pos_tagged_sentence = nltk.pos_tag(sentence.split())\n",
    "        for word, pos_tag in pos_tagged_sentence:\n",
    "            if word.lower() not in stop_words and len(word) > 1 and pos_tag.startswith(('NN', 'VB')):\n",
    "                word = lemmatizer.lemmatize(word.lower())\n",
    "                tf = tf_score(word, sentence)\n",
    "                idf = idf_score(len(sentences), word, sentences)\n",
    "                tf_idf = tf * idf\n",
    "                sentence_tf_idf += tf_idf\n",
    "        sentences_tf_idf[i] = sentence_tf_idf\n",
    "        \n",
    "    sentences_tf_idf = sorted(sentences_tf_idf.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    \n",
    "    summary = []\n",
    "    sentence_no = [x[0] for x in sentences_tf_idf[:no_of_sentences]]\n",
    "    sentence_no.sort()\n",
    "    \n",
    "    for i, sentence in enumerate(sentences, 1):\n",
    "        if i in sentence_no:\n",
    "            summary.append(sentence)\n",
    "    return \" \".join(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_rank_lsa_text(text):\n",
    "    text = text.lower()\n",
    "    sentences = sent_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    preprocessed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        sentence = re.sub(r\"[^\\w\\s]\", \"\", sentence)\n",
    "        sentence = contractions.fix(sentence)\n",
    "        tokens = word_tokenize(sentence)\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        preprocessed_sentences.append(tokens)\n",
    "    return preprocessed_sentences, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_similarity_matrix(sentences):\n",
    "    sentence_vectors = []\n",
    "    for sentence in sentences:\n",
    "        sentence_vectors.append(' '.join(sentence))\n",
    "    vectorizer = TfidfVectorizer().fit_transform(sentence_vectors)\n",
    "    similarity_matrix = cosine_similarity(vectorizer)\n",
    "    return similarity_matrix\n",
    "\n",
    "def get_text_rank_summary(custom_text, percentage):\n",
    "    processed_article,sentence_tokens = preprocess_text_rank_lsa_text(custom_text)\n",
    "    similarity_matrix = build_similarity_matrix(processed_article)\n",
    "    top_n=math.ceil(len(sentence_tokens) * (percentage/100))\n",
    "    graph = nx.from_numpy_array(similarity_matrix)\n",
    "    scores = nx.pagerank(graph)\n",
    "    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentence_tokens)), reverse=True)\n",
    "    sentence_array = [sentence[1] for sentence in ranked_sentences[:top_n]]\n",
    "    return ''.join([''.join(sentence) for sentence in sentence_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lsa_summary(custom_text, percentage):\n",
    "    processed_article,sentence_tokens = preprocess_text_rank_lsa_text(custom_text) \n",
    "    sentences = processed_article\n",
    "    num_sentences=math.ceil(len(sentence_tokens) * (percentage/100))\n",
    "    sentence_vectors = []\n",
    "    for sentence in sentences:\n",
    "        sentence_vectors.append(' '.join(sentence))\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(sentence_vectors)\n",
    "    lsa_model = TruncatedSVD(n_components=num_sentences)\n",
    "    lsa_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "    sentence_scores = lsa_matrix.sum(axis=1)\n",
    "    ranked_sentences = sorted(((sentence_scores[i], s) for i, s in enumerate(sentence_tokens)), reverse=True)\n",
    "    sentence_array = [sentence[1] for sentence in ranked_sentences[:num_sentences]]\n",
    "    return ''.join([''.join(sentence) for sentence in sentence_array])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_t5_summary(custom_text, percentage):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "    input_text = custom_text\n",
    "    total_length = len(custom_text)\n",
    "    summary_length = math.ceil(total_length * (percentage / 100))\n",
    "    \n",
    "    if total_length > 512:\n",
    "        # Divide the input text into equal parts\n",
    "        num_parts = math.ceil(total_length / 512)\n",
    "        text_parts = [input_text[i * 512:(i + 1) * 512] for i in range(num_parts)]\n",
    "        max_input_length = math.ceil(total_length/num_parts)\n",
    "        max_length = summary_length/num_parts\n",
    "        if max_length > 512:\n",
    "            max_length = 512\n",
    "        elif max_length < 56:\n",
    "            max_length = 56\n",
    "        # Generate summaries for each part\n",
    "        summaries = []\n",
    "        for part in text_parts:\n",
    "            input_ids = tokenizer.encode(part, truncation=True, max_length=max_input_length, return_tensors=\"pt\")\n",
    "            summary_ids = model.generate(input_ids, max_length=max_length)\n",
    "            summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "            summaries.append(summary)\n",
    "\n",
    "        # Combine the summaries\n",
    "        combined_summary = \" \".join(summaries)\n",
    "    else:\n",
    "        max_length = summary_length\n",
    "        if max_length > 512:\n",
    "            max_length = 512\n",
    "        elif max_length < 56:\n",
    "            max_length = 56\n",
    "    \n",
    "        input_ids = tokenizer.encode(input_text, truncation=True, max_length=total_length, return_tensors=\"pt\")\n",
    "        summary_ids = model.generate(input_ids, max_length=max_length)\n",
    "        \n",
    "        combined_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return combined_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bart_summary(custom_text, percentage):\n",
    "    model_name = 'facebook/bart-large-cnn'\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "    input_text = custom_text\n",
    "    total_length = len(custom_text)\n",
    "    summary_length = math.ceil(total_length * (percentage / 100))\n",
    "    \n",
    "    if total_length > 1024:\n",
    "        # Divide the input text into equal parts\n",
    "        num_parts = math.ceil(total_length / 1024)\n",
    "        text_parts = [input_text[i * 1024:(i + 1) * 1024] for i in range(num_parts)]\n",
    "        max_length = summary_length/num_parts\n",
    "        if max_length > 1024:\n",
    "            max_length = 1024\n",
    "        elif max_length < 56:\n",
    "            max_length = 56\n",
    "        # Generate summaries for each part\n",
    "        summaries = []\n",
    "        for part in text_parts:\n",
    "            inputs = tokenizer(part, truncation=True, padding=\"longest\", max_length=1024, return_tensors=\"pt\")\n",
    "            summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=max_length, early_stopping=True)\n",
    "            summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "            summaries.append(summary)\n",
    "\n",
    "        # Combine the summaries\n",
    "        combined_summary = \" \".join(summaries)\n",
    "    else:\n",
    "        max_length = summary_length\n",
    "        if max_length > 1024:\n",
    "            max_length = 1024\n",
    "        elif max_length < 56:\n",
    "            max_length = 56\n",
    "        inputs = tokenizer(input_text, truncation=True, padding=\"longest\", max_length=1024, return_tensors=\"pt\")\n",
    "        summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=max_length, early_stopping=True)\n",
    "        \n",
    "        combined_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "\n",
    "    return combined_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://localhost:8080/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2586f54b490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "app = dash.Dash()\n",
    "\n",
    "# Function to get the article content from the selected suggestion\n",
    "def get_article_content(suggestion_value, visible_paragraphs):\n",
    "    if suggestion_value:\n",
    "        ids = get_ids(suggestion_value)\n",
    "        article_content = '\\n\\n'.join(get_summaries(get_articles(ids[:visible_paragraphs])))\n",
    "        return article_content, ids\n",
    "    return None, None\n",
    "\n",
    "def get_updated_article_content(ids, visible_paragraphs):\n",
    "    if ids:\n",
    "        article_content = '\\n\\n'.join(get_summaries(get_articles(ids[visible_paragraphs -3 :visible_paragraphs])))\n",
    "        return article_content\n",
    "    return None\n",
    "\n",
    "# Function to split the article into paragraphs\n",
    "def split_into_paragraphs(article_content):\n",
    "    paragraphs = article_content.split('\\n\\n')\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "# Number of paragraphs to show initially\n",
    "initial_paragraphs = 3\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(children='Event based multi-document text summarisation of news articles',\n",
    "            style={'textAlign': 'center', 'color': '#000205'}),\n",
    "    html.Div(children=[\n",
    "        html.Div(children='''Vijay Jawali''', style={'textAlign': 'left', 'color': '#000205'}),\n",
    "        html.Div(children='''Project - Data Science MSc [06 32255]''', style={'textAlign': 'center', 'color': '#000205'}),\n",
    "        html.Div(children='''2437649''', style={'textAlign': 'right', 'color': '#000205'})\n",
    "    ], style={'display': 'flex', 'justifyContent': 'space-between'}),\n",
    "    html.Br(),\n",
    "    html.Hr(style={'border-top': '1px solid #000000', 'width': '100%'}),\n",
    "    html.Br(),\n",
    "\n",
    "\n",
    "    html.H3(children='Select an entity to summarise the news events using BART model',\n",
    "            style={'textAlign': 'center', 'color': '#000205'}),\n",
    "    html.H4(children='Search Entity : ',\n",
    "            style={'color': '#000205'}),\n",
    "\n",
    "    html.Div([\n",
    "        dcc.Input(\n",
    "            id='my-input',\n",
    "            type='text',\n",
    "            value='',\n",
    "            placeholder='Type here...',\n",
    "            autoComplete='off',\n",
    "            style={'float': 'left', 'width': '20%', 'margin-right': '20px'}  # Adjust width and margins\n",
    "        ),\n",
    "        html.Br(),\n",
    "        html.Br(),\n",
    "        \n",
    "        dcc.Loading(\n",
    "            id=\"loading-suggestions-container\",\n",
    "            type=\"circle\",\n",
    "            children=[html.Div(id='suggestions-container', style={'float': 'left', 'width': '20%'})],\n",
    "            style={'textAlign': 'left', 'width': '20%'}\n",
    "        ),\n",
    "    ]),\n",
    "\n",
    "    html.Div(id='entity-selected', style={'text-align': 'center', 'margin': '0 auto'}),\n",
    "\n",
    "    html.Br(),\n",
    "\n",
    "    html.Div([\n",
    "        # dcc.Markdown(\n",
    "        #     id='article-content',\n",
    "        #     children='',\n",
    "        #     style={'display': 'none'},\n",
    "        # ),\n",
    "        dcc.Loading(\n",
    "            id=\"loading-article-content\",\n",
    "            type=\"default\",\n",
    "            children=[\n",
    "                dcc.Markdown(\n",
    "                    id='article-content',\n",
    "                    children='',\n",
    "                    style={'display': 'none'},\n",
    "            )]\n",
    "        ),\n",
    "        dcc.Markdown(\n",
    "            id='article-content-updated', \n",
    "            children='',\n",
    "        ),\n",
    "        html.Br(),\n",
    "        html.Br(),\n",
    "        dcc.Loading(\n",
    "            id=\"loading-article-content-updated\",\n",
    "            type=\"default\",\n",
    "            children=[\n",
    "                dcc.Markdown(\n",
    "                    id='article-content-updated-loading',\n",
    "                    children='',\n",
    "                    style={'display': 'none'},\n",
    "                )]\n",
    "        ),\n",
    "        html.Br(),\n",
    "        html.Button('Load More', id='load-more-button', n_clicks=0, style={'margin': '10px', 'display': 'none'}),\n",
    "        \n",
    "        dcc.Store(id='hidden-paragraphs', data=[]),\n",
    "        dcc.Store(id='visible-paragraphs', data=initial_paragraphs),\n",
    "        dcc.Store(id='article-ids', data=None),\n",
    "    ], style={'float': 'right', 'width': '70%'}),\n",
    "    html.Br(),\n",
    "    html.Br(),\n",
    "    html.Br(),\n",
    "    html.Br(),\n",
    "    html.Hr(style={'border-top': '1px solid #000000', 'width': '100%', 'margin-top': '20px', 'margin-bottom': '20px'}),\n",
    "    html.Br(),\n",
    "    html.Br(),\n",
    "    \n",
    "    html.H3(children='Summarise your own text document using your own model choice',\n",
    "            style={'textAlign': 'center', 'color': '#000205'}),\n",
    "    html.H5(children='Select model',\n",
    "            style={'textAlign': 'center', 'color': '#000205'}),\n",
    "    dcc.Dropdown(\n",
    "        id='model-selection-dropdown',\n",
    "        options=[\n",
    "            {'label': 'term frequency inverse document frequency', 'value': 'term frequency inverse document frequency'},\n",
    "            {'label': 'text rank', 'value': 'text rank'},\n",
    "            {'label': 'latent semantic analysis', 'value': 'latent semantic analysis'},\n",
    "            {'label': 't5', 'value': 't5'},\n",
    "            {'label': 'bart', 'value': 'bart'},\n",
    "        ],\n",
    "        value='',  \n",
    "        style={'width': '50%', 'margin': '0 auto'}, \n",
    "    ),\n",
    "    html.H5(children='Select percentage of text to retain in the summary',\n",
    "            style={'textAlign': 'center', 'color': '#000205'}),\n",
    "    dcc.Slider(\n",
    "        id='summary-percentage-slider',\n",
    "        min=10, \n",
    "        max=90,  \n",
    "        step=1,  \n",
    "        value=None,  \n",
    "        marks={i: f\"{i}%\" for i in range(10, 91, 10)},  \n",
    "        included=False,  \n",
    "        tooltip={'placement': 'bottom'}  \n",
    "    ),\n",
    "    html.H4(id='custom-summary', style={'textAlign': 'center', 'color': '#000205', 'display': 'none'}),\n",
    "    \n",
    "    \n",
    "    html.H4(children='Enter your text : ',\n",
    "            style={'color': '#000205'}),\n",
    "    \n",
    "    dcc.Textarea(\n",
    "        id='custom-text-input',\n",
    "        placeholder='Type here...',\n",
    "        style={'width': '100%', 'height': '300px'}\n",
    "    ),\n",
    "    html.Div(id='custom-text-length-info', style={'margin-top': '10px'}),\n",
    "    \n",
    "    html.Br(),\n",
    "    html.Button('Get Summary', id='get-summary-button', n_clicks=0, style={'margin': '10px'}),\n",
    "    html.Br(),\n",
    "    html.H4(children='Summary : ',\n",
    "            style={'color': '#000205'}),\n",
    "    html.Br(),\n",
    "    dcc.Loading(\n",
    "        id=\"loading-summary-output\",\n",
    "        type=\"circle\",\n",
    "        children=[html.Div(id=\"summary-output\", style={'whiteSpace': 'pre-line'})]\n",
    "    ),\n",
    "    html.Br(),\n",
    "    html.Hr(style={'border-top': '1px solid #000000', 'width': '100%', 'margin-top': '20px', 'margin-bottom': '20px'}),\n",
    "    html.Br(),\n",
    "    html.Br(),\n",
    "    html.Br(),\n",
    "    html.Br(),\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output('suggestions-container', 'children'),\n",
    "    [Input('my-input', 'value')],\n",
    "    [State('my-input', 'id')]\n",
    ")\n",
    "def update_suggestions(value, input_id):\n",
    "    if value:\n",
    "        filtered_suggestions = [s for s in unique_entities if str(s).lower().startswith(value.lower())]\n",
    "        if filtered_suggestions:\n",
    "            # Check if the number of filtered suggestions is more than 10\n",
    "            if len(filtered_suggestions) > 10:\n",
    "                # Wrap the RadioItems in a Div with scrollable style\n",
    "                return html.Div(\n",
    "                    dcc.RadioItems(\n",
    "                        id={'type': 'suggestion', 'index': 'ALL'},\n",
    "                        options=[{'label': str(s), 'value': str(s)} for s in filtered_suggestions],\n",
    "                        labelStyle={'display': 'block', 'margin-bottom': '5px'},\n",
    "                        value=''\n",
    "                    ),\n",
    "                    style={'max-height': '500px', 'overflow': 'scroll'}  # Set the maximum height and scroll overflow\n",
    "                )\n",
    "            else:\n",
    "                # Display all suggestions if they are less than or equal to 10\n",
    "                return dcc.RadioItems(\n",
    "                    id={'type': 'suggestion', 'index': 'ALL'},\n",
    "                    options=[{'label': str(s), 'value': str(s)} for s in filtered_suggestions],\n",
    "                    labelStyle={'display': 'block', 'margin-bottom': '5px'},\n",
    "                    value=''\n",
    "                )\n",
    "    return None\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output('entity-selected', 'children'),\n",
    "    [Input({'type': 'suggestion', 'index': 'ALL'}, 'value')]\n",
    ")\n",
    "def update_output(suggestion_value):\n",
    "    if suggestion_value:\n",
    "        return html.H3('Entity Selected is: ' + str(suggestion_value),\n",
    "                       style={'textAlign': 'center', 'color': '#000205'})\n",
    "    return None\n",
    "\n",
    "@app.callback(\n",
    "    Output('article-content', 'children'),\n",
    "    Output('article-ids', 'data'),\n",
    "    Output('load-more-button', 'style'),\n",
    "    [Input({'type': 'suggestion', 'index': 'ALL'}, 'value')],\n",
    "    [State('visible-paragraphs', 'data')]\n",
    ")\n",
    "def update_article_content(suggestion_value, visible_paragraphs):\n",
    "    if suggestion_value:\n",
    "        article_content, ids = get_article_content(suggestion_value, visible_paragraphs)\n",
    "        if article_content:\n",
    "            paragraphs = split_into_paragraphs(article_content)\n",
    "            return '\\n\\n'.join(paragraphs), ids, {'display': 'block'}\n",
    "    return None, None, {'display': 'none'}\n",
    "\n",
    "@app.callback(\n",
    "    [Output('hidden-paragraphs', 'data'),\n",
    "     Output('visible-paragraphs', 'data'),\n",
    "     Output('article-content-updated', 'children'), Output('article-content-updated-loading', 'children')],\n",
    "    [Input('load-more-button', 'n_clicks'),\n",
    "    Input('article-ids', 'data')],\n",
    "    [State('hidden-paragraphs', 'data'),\n",
    "     State('visible-paragraphs', 'data'),\n",
    "     State('article-content', 'children')]\n",
    ")\n",
    "def load_more_content(n_clicks, ids, hidden_paragraphs, visible_paragraphs, article_content):\n",
    "    if article_content is None:\n",
    "        return dash.no_update, dash.no_update, dash.no_update, dash.no_update\n",
    "    if n_clicks is None:\n",
    "        return hidden_paragraphs, visible_paragraphs, None, None\n",
    "    num_visible_paragraphs =  initial_paragraphs\n",
    "    if n_clicks > 0:\n",
    "        num_visible_paragraphs = visible_paragraphs + initial_paragraphs\n",
    "        updated_article_content = get_updated_article_content(ids, num_visible_paragraphs)\n",
    "        if updated_article_content:\n",
    "            article_content =  updated_article_content\n",
    "    paragraphs = split_into_paragraphs(article_content)\n",
    "    hidden_paragraphs_to_show = hidden_paragraphs + paragraphs[-3:]\n",
    "    return hidden_paragraphs_to_show, num_visible_paragraphs, '\\n\\n'.join(hidden_paragraphs_to_show), None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output('custom-summary', 'children'),\n",
    "    Output('custom-summary', 'style'),\n",
    "    [Input('model-selection-dropdown', 'value'), \n",
    "     Input('summary-percentage-slider', 'value')],\n",
    ")\n",
    "def show_custom_selection(model, percentage):\n",
    "    if (percentage is None) or (model is None): \n",
    "        return dash.no_update, dash.no_update\n",
    "    return 'Selected model :' + str(model) + ' , ' + 'Selected percentage :' + str(percentage), {'display': 'block', 'textAlign': 'center', 'color': '#000205'}\n",
    " \n",
    "@app.callback(\n",
    "    Output('custom-text-length-info', 'children'),\n",
    "    [Input('custom-text-input', 'value')]\n",
    ")\n",
    "def update_text_length_info(text):\n",
    "    max_length = 5000\n",
    "    length = len(text) if text else 0\n",
    "    warning_style = {'color': 'red'} if length > max_length else {}\n",
    "    warning_msg = f' ({length}/{max_length})' if length > max_length else f' ({length}/{max_length})'\n",
    "    return html.Div([\n",
    "        html.Span('Text Length:', style={'font-weight': 'bold'}),\n",
    "        html.Span(warning_msg, style=warning_style)\n",
    "    ])\n",
    "    \n",
    "    \n",
    "@app.callback(\n",
    "    Output('summary-output', 'children'),\n",
    "    [Input('get-summary-button', 'n_clicks')],\n",
    "    [State('model-selection-dropdown', 'value'),\n",
    "     State('summary-percentage-slider', 'value'),\n",
    "     State('custom-text-input', 'value')]\n",
    ")\n",
    "def update_summary(n_clicks, model, percentage, custom_text):\n",
    "    if custom_text:\n",
    "        if len(custom_text) > 5000:\n",
    "            warning_style = {'color': 'red'}\n",
    "            warning_msg = \"Please provide text with less than 5000 characters.\"\n",
    "            return html.Div([\n",
    "                html.Span(warning_msg, style=warning_style)\n",
    "            ])\n",
    "    \n",
    "    if n_clicks > 0:\n",
    "        if custom_text and model and percentage:\n",
    "            \n",
    "            if model == 'term frequency inverse document frequency':\n",
    "                return get_tf_idf_summary(custom_text, percentage)\n",
    "            elif model == 'text rank':\n",
    "                return get_text_rank_summary(custom_text, percentage)\n",
    "            elif model == 'latent semantic analysis':\n",
    "                return get_lsa_summary(custom_text, percentage)\n",
    "            elif model == 't5':\n",
    "                return get_t5_summary(custom_text, percentage)\n",
    "            elif model == 'bart':\n",
    "                return get_bart_summary(custom_text, percentage)\n",
    "        else:\n",
    "            return \"Please provide all necessary inputs to generate the summary.\"\n",
    "    return ''\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(host='localhost', port=8080, debug=True, suppress_callback_exceptions=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summer_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
