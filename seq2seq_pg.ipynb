{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (C:/Users/vijay/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f955d7488e514cbe80f3697614bfa5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('cnn_dailymail', '3.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train']\n",
    "train_df = pd.DataFrame(train_data).sample(frac=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = dataset['validation']\n",
    "validation_df = pd.DataFrame(validation_data).sample(frac=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dataset['test']\n",
    "test_df = pd.DataFrame(test_data).sample(frac=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vijay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    text = ''.join([char for char in text if char.isalpha() or char.isspace()])\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token, wordnet.VERB) for token in tokens]\n",
    "\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    preprocessed_text = contractions.fix(preprocessed_text)\n",
    "\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['article'] = train_df['article'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_articles = train_df['article'].tolist()\n",
    "train_summaries = train_df['highlights'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_articles = test_df['article'].tolist()\n",
    "test_summaries = test_df['highlights'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_articles = validation_df['article'].tolist()\n",
    "validation_summaries = validation_df['highlights'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_tokenizer = Tokenizer()\n",
    "article_tokenizer.fit_on_texts(train_articles + validation_articles + test_articles)\n",
    "\n",
    "summary_tokenizer = Tokenizer()\n",
    "summary_tokenizer.fit_on_texts(train_summaries + validation_summaries + test_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_articles_seq = article_tokenizer.texts_to_sequences(train_articles)\n",
    "train_summaries_seq = summary_tokenizer.texts_to_sequences(train_summaries)\n",
    "\n",
    "validation_articles_seq = article_tokenizer.texts_to_sequences(validation_articles)\n",
    "validation_summaries_seq = summary_tokenizer.texts_to_sequences(validation_summaries)\n",
    "\n",
    "test_articles_seq = article_tokenizer.texts_to_sequences(test_articles)\n",
    "test_summaries_seq = summary_tokenizer.texts_to_sequences(test_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_max_len = 500\n",
    "summary_max_len = 50\n",
    "\n",
    "train_articles_seq = pad_sequences(train_articles_seq, maxlen=article_max_len, padding='post')\n",
    "train_summaries_seq = pad_sequences(train_summaries_seq, maxlen=summary_max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_articles_seq = pad_sequences(validation_articles_seq, maxlen=article_max_len, padding='post')\n",
    "validation_summaries_seq = pad_sequences(validation_summaries_seq, maxlen=summary_max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_articles_seq = pad_sequences(test_articles_seq, maxlen=article_max_len, padding='post')\n",
    "test_summaries_seq = pad_sequences(test_summaries_seq, maxlen=summary_max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input/output shapes and vocabulary size\n",
    "article_vocab_size = len(article_tokenizer.word_index) + 1\n",
    "summary_vocab_size = len(summary_tokenizer.word_index) + 1\n",
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(article_max_len,))\n",
    "encoder_embedding = Embedding(article_vocab_size, embedding_dim, mask_zero=True)(encoder_inputs)\n",
    "encoder_lstm1 = LSTM(128, return_sequences=True, return_state=True)\n",
    "encoder_outputs1, _, _ = encoder_lstm1(encoder_embedding)\n",
    "encoder_lstm2 = LSTM(128, return_sequences=True, return_state=True)\n",
    "encoder_outputs2, _, _ = encoder_lstm2(encoder_outputs1)\n",
    "encoder_lstm3 = LSTM(128, return_sequences=False, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm3(encoder_outputs2)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(summary_max_len,))\n",
    "decoder_embedding = Embedding(summary_vocab_size, embedding_dim, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = LSTM(128, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "# decoder_dense = TimeDistributed(Dense(summary_vocab_size, activation='softmax'))\n",
    "# decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# # Create the model\n",
    "# model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention mechanism\n",
    "attention = tf.keras.layers.Attention()([decoder_outputs, encoder_outputs])\n",
    "context_vector = tf.keras.layers.Concatenate(axis=-1)([decoder_outputs, attention])\n",
    "\n",
    "# Pointer network\n",
    "pointer_probs = TimeDistributed(Dense(1, activation='sigmoid'))(context_vector)\n",
    "pointer_probs = tf.keras.layers.Flatten()(pointer_probs)\n",
    "pointer_probs = tf.keras.layers.RepeatVector(summary_max_len)(pointer_probs)\n",
    "\n",
    "# Final probabilities\n",
    "vocab_probs = TimeDistributed(Dense(summary_vocab_size, activation='softmax'))(decoder_outputs)\n",
    "final_probs = K.concatenate([vocab_probs, pointer_probs], axis=-1)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], final_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 500)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 500, 100)     10739600    ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 500, 128),   117248      ['embedding[0][0]']              \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128)]                                                     \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 500, 128),   131584      ['lstm[0][0]']                   \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128)]                                                     \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 50, 100)      3892900     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  [(None, 128),        131584      ['lstm_1[0][0]']                 \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128)]                                                     \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  [(None, 50, 128),    117248      ['embedding_1[0][0]',            \n",
      "                                 (None, 128),                     'lstm_2[0][1]',                 \n",
      "                                 (None, 128)]                     'lstm_2[0][2]']                 \n",
      "                                                                                                  \n",
      " attention (Attention)          (None, 50, 128)      0           ['lstm_3[0][0]',                 \n",
      "                                                                  'lstm_2[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 50, 256)      0           ['lstm_3[0][0]',                 \n",
      "                                                                  'attention[0][0]']              \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 50, 1)       257         ['concatenate[0][0]']            \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 50)           0           ['time_distributed[0][0]']       \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDistri  (None, 50, 38929)   5021841     ['lstm_3[0][0]']                 \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " repeat_vector (RepeatVector)   (None, 50, 50)       0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)         (None, 50, 38979)    0           ['time_distributed_1[0][0]',     \n",
      "                                                                  'repeat_vector[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 20,152,262\n",
      "Trainable params: 20,152,262\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "135/135 [==============================] - 1614s 12s/step - loss: 7.9868 - val_loss: 7.4022\n",
      "Epoch 2/100\n",
      "135/135 [==============================] - 1347s 10s/step - loss: 6.8811 - val_loss: 7.0057\n",
      "Epoch 3/100\n",
      " 73/135 [===============>..............] - ETA: 10:42 - loss: 6.4647"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 74/135 [===============>..............] - ETA: 10:34 - loss: 6.4625"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(\n",
    "    [train_articles_seq, train_summaries_seq],\n",
    "    np.expand_dims(train_summaries_seq, axis=-1),\n",
    "    validation_data=(\n",
    "        [validation_articles_seq, validation_summaries_seq], \n",
    "        np.expand_dims(validation_summaries_seq, axis=-1)),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"seq2seq_pg.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.evaluate(\n",
    "    [test_articles_seq, test_summaries_seq],  # Use the input articles and summaries, excluding the last word\n",
    "    tf.expand_dims(test_summaries_seq, axis=-1)  # Use the target summaries, excluding the first word\n",
    ")\n",
    "\n",
    "print(\"Test Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_reverse_word_index=article_tokenizer.index_word\n",
    "summary_reverse_word_index=summary_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_predictions(articles, summaries):\n",
    "    # Convert the texts to sequences\n",
    "    article_sequences = article_tokenizer.texts_to_sequences(articles)\n",
    "    article_sequences = pad_sequences(article_sequences, maxlen=500, padding='post')\n",
    "\n",
    "    summary_sequences = summary_tokenizer.texts_to_sequences(summaries)\n",
    "    summary_sequences = pad_sequences(summary_sequences, maxlen=50, padding='post')\n",
    "\n",
    "    # Generate predictions\n",
    "    predictions = model.predict([article_sequences, summary_sequences])\n",
    "\n",
    "    # Convert predictions to text format\n",
    "    prediction_sequences = []\n",
    "    for pred in predictions:\n",
    "        pred_indices = np.argmax(pred, axis=-1)\n",
    "        prediction_sequences.append(pred_indices)\n",
    "\n",
    "    text_predictions = summary_tokenizer.sequences_to_texts(prediction_sequences)\n",
    "    \n",
    "    return text_predictions\n",
    "\n",
    "# Example usage:\n",
    "articles = [\n",
    "    \"\"\"BOGOTA, Colombia (CNN) -- A key rebel commander and fugitive from a U.S. drug trafficking indictment was killed over the weekend in an air attack on a guerrilla encampment, the Colombian military said Monday. Alleged cocaine trafficker and FARC rebel Tomas Medina Caracas in an Interpol photo. Tomas Medina Caracas, known popularly as \"El Negro Acacio,\" was a member of the high command of the Fuerzas Armadas Revolucionarias de Colombia and, according to Colombian and U.S. officials, helped manage the group's extensive cocaine trafficking network. He had been in the cross-hairs of the U.S. Justice Department since 2002. He was charged with conspiracy to import cocaine into the United States and manufacturing and distributing cocaine within Colombia to fund the FARC's 42-year insurgency against the government. U.S. officials alleged Medina Caracas managed the rebel group's sales of cocaine to international drug traffickers, who in turn smuggled it into the United States. He was also indicted in the United States along with two other FARC commanders in November 2002 on charges of conspiring to kidnap two U.S. oil workers from neighboring Venezuela in 1997 and holding one of them for nine months until a $1 million ransom was paid. Officials said the army's Rapid Response Force, backed by elements of the Colombian Air Force, tracked Medina Caracas down at a FARC camp in the jungle in the south of the country. \"After a bombardment, the troops occupied the camp, and they've found 14 dead rebels so far, along with rifles, pistols, communications equipment and ... four GPS systems,\" Defense Minister Juan Manuel Santos said at a news conference. \"The death of 'El Negro Acacio' was confirmed by various sources, including members of FARC itself.\" Medina Caracas commanded FARC's 16th Front in the southern departments of Vichada and Guainia. Established in 1964 as the military wing of the Colombian Communist Party, FARC is Colombia's oldest, largest, most capable and best-equipped Marxist rebel group, according to the U.S. Department of State. E-mail to a friend . Journalist Fernando Ramos contributed to this report.\"\"\"\n",
    "]\n",
    "\n",
    "summaries = [\n",
    "    \"\"\"Tomas Medina Caracas was a fugitive from a U.S. drug trafficking indictment . \"El Negro Acacio\" allegedly helped manage extensive cocaine network . U.S. Justice Department indicted him in 2002 . Colombian military: He was killed in an attack on a guerrilla encampment .\"\"\"\n",
    "]\n",
    "\n",
    "predictions = generate_text_predictions(articles, summaries)\n",
    "for text, summ, pred in zip(articles, summaries, predictions):\n",
    "    print(\"Article:\", text)\n",
    "    print(\"Golden Summary:\", summ)\n",
    "    print(\"Prediction Summary:\", pred)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # Retrieve individual text document summaries\n",
    "# summary1 = \"Summary of document 1.\"\n",
    "# summary2 = \"Summary of document 2.\"\n",
    "# summary3 = \"Summary of document 3.\"\n",
    "\n",
    "# # Combine summaries\n",
    "# combined_summary = []\n",
    "\n",
    "# # Tokenize and calculate sentence importance using TF-IDF\n",
    "# summaries = [summary1, summary2, summary3]\n",
    "# tfidf_vectorizer = TfidfVectorizer()\n",
    "# tfidf_matrix = tfidf_vectorizer.fit_transform(summaries)\n",
    "\n",
    "# # Calculate importance scores for each sentence\n",
    "# sentence_scores = tfidf_matrix.sum(axis=0)\n",
    "\n",
    "# # Sort sentences based on importance scores\n",
    "# top_sentences_indices = sentence_scores.argsort()[0, ::-1]\n",
    "\n",
    "# # Select top N sentences for combined summary\n",
    "# N = 3  # Number of sentences to select\n",
    "# top_sentences = [sentences[i] for i in top_sentences_indices[:N]]\n",
    "\n",
    "# # Join selected sentences to form the final combined summary\n",
    "# combined_summary = ' '.join(top_sentences)\n",
    "\n",
    "# # Perform post-processing if needed\n",
    "\n",
    "# # Print the combined summary\n",
    "# print(combined_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.models import load_model\n",
    "\n",
    "# def generate_text_predictions(articles, summaries):\n",
    "#     model = load_model('text_generation_model.h5')  # Load the trained model\n",
    "#     summary_tokenizer = Tokenizer()  # Create a new tokenizer for the summaries\n",
    "#     summary_tokenizer.fit_on_texts(summaries)  # Fit the tokenizer on the summary texts\n",
    "#     text_predictions = []\n",
    "\n",
    "#     for article in articles:\n",
    "#         sequences = summary_tokenizer.texts_to_sequences([article])  # Convert article text to sequences\n",
    "#         input_data = pad_sequences(sequences, maxlen=1000)  # Pad the input sequence\n",
    "#         pred = model.predict(input_data)  # Generate predictions\n",
    "#         pred_indices = np.argmax(pred, axis=-1)\n",
    "#         pred_indices_list = pred_indices.tolist()  # Convert numpy array to list\n",
    "#         pred_text = summary_tokenizer.sequences_to_texts(pred_indices_list)\n",
    "#         text_predictions.append(pred_text)\n",
    "\n",
    "#     return text_predictions\n",
    "\n",
    "# # Example usage\n",
    "# articles = [\n",
    "#     \"\"\"BOGOTA, Colombia (CNN) -- A key rebel commander and fugitive from a U.S. drug trafficking indictment was killed over the weekend in an air attack on a guerrilla encampment, the Colombian military said Monday. Alleged cocaine trafficker and FARC rebel Tomas Medina Caracas in an Interpol photo. Tomas Medina Caracas, known popularly as \"El Negro Acacio,\" was a member of the high command of the Fuerzas Armadas Revolucionarias de Colombia and, according to Colombian and U.S. officials, helped manage the group's extensive cocaine trafficking network. He had been in the cross-hairs of the U.S. Justice Department since 2002. He was charged with conspiracy to import cocaine into the United States and manufacturing and distributing cocaine within Colombia to fund the FARC's 42-year insurgency against the government. U.S. officials alleged Medina Caracas managed the rebel group's sales of cocaine to international drug traffickers, who in turn smuggled it into the United States. He was also indicted in the United States along with two other FARC commanders in November 2002 on charges of conspiring to kidnap two U.S. oil workers from neighboring Venezuela in 1997 and holding one of them for nine months until a $1 million ransom was paid. Officials said the army's Rapid Response Force, backed by elements of the Colombian Air Force, tracked Medina Caracas down at a FARC camp in the jungle in the south of the country. \"After a bombardment, the troops occupied the camp, and they've found 14 dead rebels so far, along with rifles, pistols, communications equipment and ... four GPS systems,\" Defense Minister Juan Manuel Santos said at a news conference. \"The death of 'El Negro Acacio' was confirmed by various sources, including members of FARC itself.\" Medina Caracas commanded FARC's 16th Front in the southern departments of Vichada and Guainia. Established in 1964 as the military wing of the Colombian Communist Party, FARC is Colombia's oldest, largest, most capable and best-equipped Marxist rebel group, according to the U.S. Department of State. E-mail to a friend . Journalist Fernando Ramos contributed to this report.\"\"\"\n",
    "# ]\n",
    "\n",
    "# summaries = [\n",
    "#     \"\"\"Tomas Medina Caracas was a fugitive from a U.S. drug trafficking indictment . \"El Negro Acacio\" allegedly helped manage extensive cocaine network . U.S. Justice Department indicted him in 2002 . Colombian military: He was killed in an attack on a guerrilla encampment .\"\"\"\n",
    "# ]\n",
    "\n",
    "# predictions = generate_text_predictions(articles, summaries)\n",
    "\n",
    "# for text, summ, pred in zip(articles, summaries, predictions):\n",
    "#     print(\"Article:\", text)\n",
    "#     print(\"Summary:\", summ)\n",
    "#     print(\"Predicted Summary:\", pred)\n",
    "#     print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
